{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bandi-Lavanya/FMML_2023_ASSIGNMENTS/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803bf5d7-4e84-40cd-d64f-9aaa60ccd437"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed30c703-0c6b-44c8-93fa-f0f368b51acf"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c46eca-2f81-4b64-b296-4b913c5608e3"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c7ffd0-fc0c-428e-d69e-91884a2ab9e9"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ccc3928-0b01-4105-cdb7-8d1930094ca0"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f531a20-235c-47cc-e39d-cf82218d21cc"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "\n",
        "**ANSWER:**\n",
        "*The percentage of data allocated to the validation set in a machine learning experiment can significantly impact the accuracy of the validation set and, by extension, the model development and evaluation process.\n",
        "\n",
        "**Affectes if we increase the percentage of validation set when we reduce the data allocate dto it:**\n",
        "\n",
        "**Increase Percentage of Validation Set:**\n",
        "\n",
        "**1.Positive Impact on Validation Accuracy**: When we allocate a larger percentage of our data to the validation set, we are effectively using more data to evaluate our model's performance. This can lead to a more accurate estimate of our model's performance on unseen data, resulting in a higher validation accuracy.\n",
        "\n",
        "**2.Reduced Training Data**: One trade-off is that you'll have less data available for training your model. With a smaller training set, your model may not learn as effectively from the data, and it might be prone to overfitting, especially if the dataset is small to begin with.\n",
        "\n",
        "**Reduce Percentage of Validation Set:**\n",
        "\n",
        "**1.Positive Impact on Training Accuracy:** Allocating a smaller percentage to the validation set means you have more data available for training. This can lead to a model that learns more effectively from the training data and achieves higher training accuracy.\n",
        "\n",
        "**2.Lower Validation Accuracy**: On the downside, a smaller validation set may result in a less reliable estimate of your model's performance. With less data for validation, your estimate may be more susceptible to random variations in the validation set, potentially leading to a lower validation accuracy."
      ],
      "metadata": {
        "id": "TPDl7M_szc0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "\n",
        "**ANSWER:**\n",
        "The size of the training and validation sets can indeed affect how well we can predict the accuracy on the test set using the validation set.\n",
        "\n",
        "**Training Set Size:**\n",
        "\n",
        "**Larger Training Set:**\n",
        "* It results in a better-trained model because it has more data to learn from.\n",
        "* It can lead to a more accurate prediction of test set performance using the validation set.\n",
        "**Smaller Training Set:**\n",
        "* with a smaller training set, your model may not learn as effectively, potentially leading to overfitting.\n",
        "* In such cases, the validation set's accuracy may not be a good predictor of test set accuracy.\n",
        "\n",
        "**Validation Set Size:**\n",
        "\n",
        "**Larger Validation Set:**\n",
        "* A larger validation set can provide a more reliable estimate of your model's performance because it's based on more data.\n",
        "* The validation set's accuracy is a better predictor of the test set accuracy.\n",
        "\n",
        "**Smaller Validation Set:**\n",
        "* A smaller validation set can still provide valuable insights into model performance, but it may be more sensitive to variations in the data.\n",
        "* It could lead to less accurate predictions of test set accuracy."
      ],
      "metadata": {
        "id": "gvgxsk1EBzRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "**ANSWER**:\n",
        "* I think good percentage to reserve for the validation set is typically around \"20-30%\" so that thest two factors are balanced.\n",
        "* There are some reasons like common practices , balancedtrade-off and overfitting for reserving around 20-30% ."
      ],
      "metadata": {
        "id": "TNYm0thqDuFB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd527f6-0640-4de7-b446-4e33d6be7069"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "\n",
        "**ANSWER:**\n",
        "Yes,averaging the validation accuracy across multiple splits give more consistent results.\n",
        "*cross-validation helps to reduce the impact of data randomness and variability in your model's performance evaluation.\n",
        "\n",
        "**Reasons for more consistent reasults:**\n",
        "\n",
        "**1.Robustness to Data Variability:**\n",
        "*   Inherent variability in data sets which is used to variation in data distribution\n",
        "* By using multiple splits, you get a better sense of how well your model generalizes to different subsets of the data.  \n",
        "\n",
        "**2.Reduced Overfitting:**\n",
        "*   The averaging of results over multiple splits helps to reduce ovrfitting.\n",
        "*   Using single validation to evaluate our model,it might perform exceptionally well or poorly due to the luck of the split, leading to an inaccurate representation of its actual performance.\n",
        "  \n",
        "**3.Improved Confidence Estimation:**\n",
        "*   The averaging of results over multiple splits helps to improved confidential estimation.\n",
        "*   It also reduces the impact of noise in a single validation set.\n",
        "\n",
        "**4.Consistency:**\n",
        "*   When you repeatedly split the data and evaluate your model's performance, you can observe how consistent its results are across different subsets.   \n",
        "\n"
      ],
      "metadata": {
        "id": "0wGCbgfBy1jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Does it give more accurate estimate of test accuracy?\n",
        "\n",
        "**ANSWER:**\n",
        "*   Averaging the validation accuracy across multiple splits in cross-validation does not directly provide a more accurate estimate of the test accuracy.\n",
        "*   This estimate is often referred to as the **\"cross-validated performance\" or \"cross-validated accuracy.\"**\n",
        "*   Purpose of cross-validation: It helps you evaluate the model's performance more reliably and provides insights into its ability to handle different data subsets.\n",
        "*   cross-validation can give a good estimate of  model's generalization performance, but it doesn't directly provide the test accuracy.\n",
        "*   The test dataset remains crucial for confirming your model's performance before deploying it in real-world applications, as it serves as the ultimate benchmark of its accuracy on new, independent samples.\n",
        "\n",
        "*The example below serves as the closest estimate to the actual test accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "9YJavG033ASb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into a training set and a test set (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation on the training set\n",
        "k = 5\n",
        "cross_val_scores = cross_val_score(model, X_train, y_train, cv=k, scoring='accuracy')\n",
        "\n",
        "# Calculation of the average cross-validated accuracy\n",
        "average_accuracy = sum(cross_val_scores) / k\n",
        "\n",
        "# Training the model on the entire training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the test accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the cross-validated accuracy and test accuracy\n",
        "print(\"The Cross-Validated Accuracy:\", average_accuracy)\n",
        "print(\"The Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDsX0-HUAbjj",
        "outputId": "4ec7d555-5805-4e62-b14b-087fb62c275e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validated Accuracy: 0.9416666666666668\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "\n",
        "**ANSWER:**\n",
        "The number of iterations or folds in cross-validation can have an effect on the estimate of your model's performance. The impact of the number of iterations on the estimate depends on several factors:\n",
        "\n",
        "**Data Availability:**\n",
        " If you have a small dataset, using a very high number of folds might not be feasible, and you could end up with very small training and validation sets in each fold.\n",
        "\n",
        " **Bias-Variance Trade-Off:**\n",
        " 1. With a higher number of folds (e.g., k-fold cross-validation with a large k), us reduce the bias in our estimate because our're using more data for both training and validation.\n",
        " 2. This can lead to higher variance in our estimate because each fold represents a smaller portion of our data, which can make the estimate more sensitive to random fluctuations.\n",
        "\n",
        " **Computational Cost:**\n",
        " Increasing the number of iterations in cross-validation can significantly increase the computational cost, especially for large datasets and complex models.\n",
        "\n",
        " * Increasing the number of iterations (folds) in cross-validation can lead to a better estimate of your model's performance, but it comes with trade-offs.\n",
        " * The effect of higher iterations on the estimate depends on various factors, including your dataset size, the complexity of your model, and computational resources."
      ],
      "metadata": {
        "id": "2kS5nqyl3b1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset (a well-known dataset for classification)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define a decision tree classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Try different numbers of folds (iterations)\n",
        "fold_values = [3, 5, 10]\n",
        "for folds in fold_values:\n",
        "    # Perform cross-validation\n",
        "    scores = cross_val_score(model, X, y, cv=folds, scoring='accuracy')\n",
        "\n",
        "    # Calculate and print the average accuracy\n",
        "    average_accuracy = sum(scores) / len(scores)\n",
        "    print(f'{folds}-Fold Cross-Validation - Average Accuracy: {average_accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chfOhm8gyJs3",
        "outputId": "a24eacd7-db19-48d7-c0f3-2a2c87488baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-Fold Cross-Validation - Average Accuracy: 0.96\n",
            "5-Fold Cross-Validation - Average Accuracy: 0.95\n",
            "10-Fold Cross-Validation - Average Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "\n",
        "**ANSWER**:\n",
        "* Increasing the iterations in cross-validation can help mitigate the impact of a very small train or validation dataset to some extent, but it may not fully compensate for the limitations of extremely small datasets.\n",
        "* Instead, it repeatedly partitions your existing dataset into training and validation subsets, allowing you to assess your model's performance multiple times with different data splits."
      ],
      "metadata": {
        "id": "3KiWYDuPy0zM"
      }
    }
  ]
}